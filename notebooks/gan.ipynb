{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd, numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/global/cfs/cdirs/m3443/usr/pmtuan/hadsim')\n",
    "import yaml\n",
    "from data.utils import *\n",
    "from data.datamodule import CartesianDataModule\n",
    "\n",
    "DATA_PATH = \"/global/cfs/cdirs/m3443/usr/pmtuan/HadronicMCData/train_data_2_particles_processed/\"\n",
    "data_files = os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'n_particle': 1,\n",
    "    'max_etot': 100000,\n",
    "    'min_etot': 10000,\n",
    "    'gen_hidden_activation': 'LeakyReLU',\n",
    "    'dis_hidden_activation': 'LeakyReLU',\n",
    "    'gen_output_activation': 'Tanh',\n",
    "    'dis_output_activation': 'Sigmoid',\n",
    "    'gen_batchnorm': True,\n",
    "    'dis_batchnorm': True,\n",
    "    'gen_dropout_rate': 0.5,\n",
    "    'dis_dropout_rate': 0.,\n",
    "    'nb_gen_layer': 10,\n",
    "    'nb_dis_layer': 10,\n",
    "    'gen_lr': 0.001,\n",
    "    'dis_lr': 0.001,\n",
    "    \n",
    "    'sort_by': 0,\n",
    "    'batch_size': 8096,\n",
    "    'input_dir': '/global/cfs/cdirs/m3443/usr/pmtuan/HadronicMCData/2_particle_fstate',\n",
    "    'hidden':  128,\n",
    "    \n",
    "    'noise_dim': 4,\n",
    "    'cond_dim': 1,\n",
    "    'gen_in': 4,\n",
    "    'gen_dim': 4,\n",
    "    'data_module': 'CartesianDataModule'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import nn\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class Generator(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.network = make_mlp(\n",
    "            input_size=self.hparams['noise_dim'] + self.hparams['cond_dim'],\n",
    "            sizes=[self.hparams['hidden']] * self.hparams['nb_gen_layer'] + [self.hparams['gen_dim']],\n",
    "            hidden_activation=hparams['gen_hidden_activation'],\n",
    "            output_activation=hparams['gen_output_activation'],\n",
    "            dropout_rate=self.hparams['gen_dropout_rate'],\n",
    "            batch_norm=self.hparams['gen_batchnorm']\n",
    "        )\n",
    "\n",
    "    def forward(self, cond):\n",
    "        noise = torch.distributions.normal.Normal(0., 1.).sample(torch.Size([cond.shape[0], self.hparams['noise_dim']])).to(self.device)\n",
    "        z = torch.cat([cond, noise], dim=-1)\n",
    "        return self.network(z)\n",
    "\n",
    "class Discriminator(LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.network = make_mlp(\n",
    "            input_size=self.hparams['gen_dim'] + self.hparams['cond_dim'],\n",
    "            sizes=[self.hparams['hidden']] * self.hparams['nb_dis_layer'] + [1],\n",
    "            hidden_activation=hparams['dis_hidden_activation'],\n",
    "            output_activation=hparams['dis_output_activation'],\n",
    "            dropout_rate=self.hparams['dis_dropout_rate'],\n",
    "            batch_norm=self.hparams['dis_batchnorm']\n",
    "        )\n",
    "\n",
    "    def forward(self, cond, x):\n",
    "        z = torch.cat([cond, x], dim=-1)\n",
    "        return self.network(z)\n",
    "\n",
    "class GAN(LightningModule):\n",
    "    def __init__(self, hparams) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.gen, self.dis = Generator(self.hparams), Discriminator(self.hparams)\n",
    "\n",
    "        self.data_module = eval(self.hparams['data_module'])(self.hparams)\n",
    "    \n",
    "    def setup(self, stage = 'fit') -> None:\n",
    "        self.data_module.setup(stage)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.data_module.train_dataloader()\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.data_module.val_dataloader()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.data_module.test_dataloader()\n",
    "\n",
    "    def forward(self, cond):\n",
    "        return self.gen(cond)\n",
    "    \n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return nn.functional.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        cond, _, truth = batch\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            # generate data\n",
    "            fake = self(cond)\n",
    "\n",
    "            # create fake label that looks like truth label\n",
    "            y = torch.ones(cond.size(0), 1, device=self.device)\n",
    "\n",
    "            y_hat = self.dis(cond, fake)\n",
    "\n",
    "            g_loss = self.adversarial_loss(y_hat, y)\n",
    "\n",
    "            self.log('gen_loss', g_loss, prog_bar=True, on_step=False, on_epoch=True, batch_size=self.hparams['batch_size'])\n",
    "\n",
    "            return g_loss\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            # train discriminator\n",
    "            truth_label = torch.ones(truth.size(0), 1, device=self.device)\n",
    "\n",
    "            fake = self(cond).detach()\n",
    "\n",
    "            # create actual fake label\n",
    "            fake_label = torch.zeros(cond.size(0), 1, device=self.device)\n",
    "\n",
    "            y_hat = self.dis(\n",
    "                torch.cat([cond, cond], dim=0),\n",
    "                torch.cat([truth, fake], dim=0)                \n",
    "            )\n",
    "            \n",
    "            y = torch.cat([truth_label, fake_label], dim=0)\n",
    "\n",
    "            d_loss = self.adversarial_loss( y_hat, y )\n",
    "\n",
    "            self.log('dis_loss', d_loss, prog_bar=True, on_step=False, on_epoch=True, batch_size=self.hparams['batch_size'])\n",
    "\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(self.gen.parameters(), lr=self.hparams['gen_lr'])\n",
    "        opt_d = torch.optim.Adam(self.dis.parameters(), lr=self.hparams['dis_lr'])\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def shared_evaluation(self, batch, batch_idx, log=True):\n",
    "        cond, _, truth = batch\n",
    "        \n",
    "        fake = self(cond)\n",
    "\n",
    "        y = torch.ones(cond.size(0), 1, device=self.device)\n",
    "\n",
    "        y_hat = self.dis(cond, fake)\n",
    "\n",
    "        loss = self.adversarial_loss(y_hat, y)\n",
    "\n",
    "        self.log_metrics(cond, truth, fake)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "\n",
    "    def log_metrics(self, cond, truth, fake):\n",
    "\n",
    "        nominal_cond = torch.round(cond, decimals=1).view(-1)\n",
    "        unique_cond = nominal_cond.unique()\n",
    "\n",
    "        w_dis = {}\n",
    "        for u_cond in nominal_cond.unique():\n",
    "            mask = (nominal_cond == u_cond)\n",
    "            c_truth, c_fake = truth[mask], fake[mask]\n",
    "            w_dis[u_cond] = np.mean(\n",
    "                [\n",
    "                    wasserstein_distance(c_truth.detach().cpu().numpy()[:, idx], c_fake.detach().cpu().numpy()[:, idx]) for idx in range(c_truth.size(1))\n",
    "                ]\n",
    "            )\n",
    "        mean_wdis = np.mean(list(w_dis.values()))\n",
    "\n",
    "        self.log_dict({\n",
    "            \"mean_wdis\" : mean_wdis,\n",
    "\n",
    "        },\n",
    "        on_epoch=True, on_step=False, prog_bar=True, batch_size=self.hparams['batch_size'] )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.shared_evaluation(batch, batch_idx)\n",
    "        return outputs['loss']\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self.shared_evaluation(batch, batch_idx)\n",
    "        return outputs['loss']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 1/1  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">321/321</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:11 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">4.07it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 0.544 v_num: 3668609        </span>\n",
       "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">mean_wdis: 0.61 gen_loss: 0.792   </span>\n",
       "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">dis_loss: 0.319                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 1/1  \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m321/321\u001b[0m \u001b[38;5;245m0:01:11 • 0:00:00\u001b[0m \u001b[38;5;249m4.07it/s\u001b[0m \u001b[37mloss: 0.544 v_num: 3668609        \u001b[0m\n",
       "                                                                                 \u001b[37mmean_wdis: 0.61 gen_loss: 0.792   \u001b[0m\n",
       "                                                                                 \u001b[37mdis_loss: 0.319                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\n",
    "\n",
    "gan = GAN(hparams)\n",
    "trainer = Trainer(max_epochs=2, callbacks=[RichProgressBar(refresh_rate=10)], accelerator='auto', devices=1)\n",
    "\n",
    "trainer.fit(gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsim_torch",
   "language": "python",
   "name": "hadsim_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e83746e3048c1e7681ea3da4fb484a227c1ffd1dae52935990ec24d7b2045666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
